---
title: "TIBs-CDSB-Mapping"
author: "Diana Oaxaca y Mirna Vazquez Rosas Landa"
date: '2022-07-11'
output: html_document
---

### Mapeo

3. **Profundidad**: La profundidad de cada contig generado se realizó mediante el mapeo de las lecturas al ensamble. Este paso permite evaluar la calidad del ensable y es necesario para hacer la reconstrucción de genomas ya que, como veremos más adelante, es uno de los parámetros que toman en cuenta los "bineadores". El mapeo se realizó con la herramienta BBMap del programa **[BBtools](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/)**.


```{bash eval=F}
mkdir -p 01.Mapeo/{data,results,logs}
cd 01.Mapeo/
ln -s /home/diana/samples/htn/data/htn.fasta data/  
ln -s /home/diana/samples/htn/data/htn*.fastq  data/ 
```


```{bash eval=F}
bash /home/programs/bbmap/bbmap.sh ref=data/htn.fasta in1=data/htn_1.fastq in2=data/htn_2.fastq out=data/htn.sam kfilter=22 subfilter=15 maxindel=80 bamscript=src/sam2bam.sh covstats=data/htn.covstats scafstats=data/htn.scafstats
bash src//sam2bam.sh
```

### Binning

Utilizaremos varios programas para hacer la reconstrucción de los genomas y haremos una comparación de estos.

**NOTA**: Cada programa tiene una ayuda y un manual de usuario, es **importante** revisarlo y conocer cada parámetro que se ejecute. En terminal se puede consultar el manual con el comando `man` y también se puede consultar la ayuda con `-h` o `--help`, por ejemplo `fastqc -h`.

La presente práctica sólo es una representación del flujo de trabajo, sin embargo, no sustituye los manuales de cada programa y el flujo puede variar dependiendo del tipo de datos y pregunta de investigación.

**¡Manos a la obra!**

Conéctate al servidor:

```{bash eval=F}
ssh USER@hpc-matematicas-z.fciencias.unam.mx
```

#### MaxBin

Crea tu espacio de trabajo y una liga símbólica hacia los datos que se usarán:

```{bash eval=F}
mkdir -p 02.MaxBin/{data,results,logs}
cd 02.MaxBin/
ln -s /home/diana/samples/htn/data/htn.fasta data/  
ln -s /home/diana/samples/htn/data/htn-depth.txt  data/ 
```

```{bash run MaxBin2, eval=FALSE}
run_MaxBin.pl -contig data/htn.fasta -out results/maxbin/ -abund data/htn-depth.txt
```

#### MetaBat

```{bash eval=F}
mkdir -p 03.Metabat/{data,results,logs}
cd 03.Metabat/
ln -s /home/diana/samples/htn/data/htn.fasta data/  
ln -s /home/diana/samples/htn/data/htn_sorted.bam  data/ 
```

Para MetaBat lo primero que tenemos que hacer es crear un archivo de profundidad utilizando el script **jgi_summarize_bam_contig_depths**.

Entonces primero activamos el ambinte.

```{bash eval=FALSE}
conda activate metabat
```

Como cualquier otro programa **jgi_summarize_bam_contig_depths** tiene opciones, podemos revisarlas. 

```{bash eval=FALSE}
jgi_summarize_bam_contig_depths --outputDepth htn-depth.txt htn_sorted.bam
```

Okay... exploremos el archivo con **head**

```{bash eval=FALSE}
head htn-depth.txt
```

Para metabat solo necesitamos dos archivos principales:

- El ensamble
- El archivo de profundidad

El resto de argumentos que vamos a usar se refieren a:

 - minCVSum: Cobertura media  total mínima de un contig (suma de profundidad)
 - saveCls: guardar membresías de clúster como un formato de matriz
 - d: salida corta
 - v: salida detallada
 - minCV: Cobertura media mínima de un contig en cada biblioteca para binning.
 - m: Tamaño mínimo de un contig para binning (debe ser >=1500). Usualmente usamos 2000 pero para el ejercicio usaremos 1500
 
```{bash running Metabat, eval=FALSE}
metabat -i htn.fasta -a /home/mirna/03.Mapeo/htn-depth.txt -o bins -t 1 --minCVSum 0 --saveCls -d -v --minCV 0.1 -m 1500
```

#### CONCOCT

```{bash eval=F}
mkdir -p 04.Concoct/{data,results,logs}
cd 04.Concoct/
ln -s /home/diana/samples/htn/data/htn.fasta data/  
ln -s /home/diana/samples/htn/data/htn_sorted.bam  data/ 
```

Primero activemos el ambiente

```{bash eval=FALSE}
conda activate concoct_env
```

Primero los contigs se tienen que partir en pedazos mas pequeños

```{bash split assembly, eval=FALSE}
cut_up_fasta.py htn.fasta -c 10000 -o 0 --merge_last -b SplitAssembly-htn.bed > htn.fasta-split10K.fa
```

Para creas la tabla de cobertura se necesita primero indexar el archivo bam

```{bash index bamfile, eval=FALSE}
samtools index htn_sorted.bam
```

```{bash create coverage table, eval=FALSE}
concoct_coverage_table.py SplitAssembly-htn.bed htn_sorted.bam > concoct_coverage_table_htn.tsv
```

Ahora si!! a correr concoct.

Normalmente correriamos 500 iteraciones, pero esta vez solo haremos una.

```{bash run concot, eval=FALSE}
concoct --coverage_file concoct_coverage_table_htn.tsv --composition_file htn.fasta-split10K.fa --clusters 400 --kmer_length 4 --threads 4 --length_threshold 3000 --basename concot --seed 4 --iterations 1
```

Merge subcontig clustering into original contigs

```{bash merge step, eval=FALSE}
merge_cutup_clustering.py concot_clustering_gt3000.csv > merged-htn-gt3000.csv
```

Extract bins as individual fasta.

```{bash make fastafiles, eval=FALSE}
mkdir bins-concot
extract_fasta_bins.py  htn.fasta merged-htn-gt3000.csv --output_path bins-concot
```

### Refinamiento

#### DASTool
