---
title: "TIBs-CDSB-Binning"
author: "Diana Oaxaca y Mirna Vazquez Rosas Landa"
date: '2022-07-11'
output: html_document
---

### Mapeo

**Profundidad**: La profundidad de cada contig se calcula mapeando las lecturas al ensamble. Este paso permite evaluar la calidad del ensable y es necesario para hacer la reconstrucción de genomas ya que, como veremos más adelante, es uno de los parámetros que toman en cuenta los "bineadores". 

Vamos a mapear utilizando la herramienta BBMap del programa **[BBtools](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/)**. Y samtools. 

**¡Manos a la obra!**

Conéctate al servidor:

```{bash eval=F}
ssh USER@hpc-matematicas-z.fciencias.unam.mx
```

Primero crea tu carpera y una liga simbolica a los datos:

```{bash acomodando archivos eval=F}
mkdir -p 01.Mapeo/{data,results}
cd 01.Mapeo/
ln -s /home/diana/samples/htn/data/htn.fasta data/  
ln -s /home/mirna/03.Mapeo/01.Trimm_reads/htn*.fastq  data/ 
```

Primero que nada activa tu ambiente de conda.

```{bash  eval=F}
conda activate bbmap_env
```

Ahora ¡sí! explora las opciones de bbmap, y vamos a hacer nuestro primer mapeo.

```{bash mapeando eval=F}
bbmap.sh ref=data/htn.fasta in1=data/htn_1-short.fastq in2=data/htn_2-short.fastq out=results/htn.sam kfilter=22 subfilter=15 maxindel=80 
```

```{bash creando bam file eval=F}
cd results
samtools view -bShu htn.sam | samtools sort -@ 94 -o htn_sorted.bam
samtools index htn_sorted.bam
```

# Discutamos

https://docs.google.com/document/d/1iiw-q-90nATg-RNTd9nU8L1XE5xoDRC6j19JC1GiPIk/edit?usp=sharing

### Binning

Utilizaremos varios programas para hacer la reconstrucción de los genomas y haremos una comparación de estos.

**NOTA**: Cada programa tiene una ayuda y un manual de usuario, es **importante** revisarlo y conocer cada parámetro que se ejecute. En terminal se puede consultar el manual con el comando `man` y también se puede consultar la ayuda con `-h` o `--help`, por ejemplo `fastqc -h`.

La presente práctica sólo es una representación del flujo de trabajo, sin embargo, no sustituye los manuales de cada programa y el flujo puede variar dependiendo del tipo de datos y pregunta de investigación.

#### MaxBin

Crea tu espacio de trabajo y una liga símbólica hacia los datos que se usarán:

```{bash eval=F}
mkdir -p 02.MaxBin/{data,results}
cd 02.MaxBin/
ln -s /home/diana/samples/htn/data/htn.fasta data/  
ln -s /home/diana/samples/htn/data/htn-depth.txt  data/ 
```

Okay, ahora activa tu ambiente.

```{bash, eval=False}
conda avtivate maxbin_env
```

Explora las opciones y luego ahora si, a calcular bins. 

```{bash run MaxBin2, eval=FALSE}
run_MaxBin.pl -contig data/htn.fasta -out results/maxbin -abund data/htn-depth.txt -max_iteration 2
```

#### MetaBat

Okay vamos a utilizar otro progama. Crea tus ligas simbolicas :)

```{bash eval=F}
mkdir -p 03.Metabat/{data,results}
cd 03.Metabat/
ln -s /home/diana/samples/htn/data/htn.fasta data/  
ln -s /home/diana/samples/htn/data/htn_sorted.bam  data/ 
```

Para MetaBat lo primero que tenemos que hacer es crear un archivo de profundidad utilizando el script **jgi_summarize_bam_contig_depths**.

Entonces primero activamos el ambinte.

```{bash eval=FALSE}
conda activate metabat_env
```

Como cualquier otro programa **jgi_summarize_bam_contig_depths** tiene opciones, podemos revisarlas. 

```{bash eval=FALSE}
jgi_summarize_bam_contig_depths --outputDepth data/htn-depth.txt data/htn_sorted.bam
```

Okay... exploremos el archivo con **head**

```{bash eval=FALSE}
head data/htn-depth.txt
```

Para metabat solo necesitamos dos archivos principales:

- El ensamble
- El archivo de profundidad
 
```{bash running Metabat, eval=FALSE}
metabat -i data/htn.fasta -a data/htn-depth.txt -o results/bins -t 4 --minCVSum 0 --saveCls -d -v --minCV 0.1 -m 2000
```

#### CONCOCT

Okay vamos a utilizar otro progama. Crea tus ligas simbolicas :)

```{bash eval=F}
mkdir -p 04.Concoct/{data,results}
cd 04.Concoct/
ln -s /home/diana/samples/htn/data/htn.fasta data/  
ln -s /home/diana/samples/htn/data/htn_sorted.bam  data/ 
```

Primero activemos el ambiente

```{bash eval=FALSE}
conda activate concoct_env
```

Primero los contigs se tienen que partir en pedazos mas pequeños

```{bash split assembly, eval=FALSE}
cut_up_fasta.py data/htn.fasta -c 10000 -o 0 --merge_last -b results/SplitAssembly-htn.bed > results/htn.fasta-split10K.fa
```

Para creas la tabla de cobertura se necesita primero indexar el archivo bam

```{bash index bamfile, eval=FALSE}
samtools index data/htn_sorted.bam
```

```{bash create coverage table, eval=FALSE}
concoct_coverage_table.py results/SplitAssembly-htn.bed data/htn_sorted.bam > results/concoct_coverage_table_htn.tsv
```

Ahora si!! a correr concoct.

Normalmente correriamos 500 iteraciones, pero esta vez solo haremos una.

```{bash run concot, eval=FALSE}
concoct --coverage_file results/concoct_coverage_table_htn.tsv --composition_file results/htn.fasta-split10K.fa --clusters 400 --kmer_length 4 --threads 4 --length_threshold 3000 --basename concot --seed 4 --iterations 1
```

Merge subcontig clustering into original contigs

```{bash merge step, eval=FALSE}
merge_cutup_clustering.py concot_clustering_gt3000.csv > results/merged-htn-gt3000.csv
```

Extract bins as individual fasta.

```{bash make fastafiles, eval=FALSE}
mkdir results/bins-concot
extract_fasta_bins.py  data/htn.fasta results/merged-htn-gt3000.csv --output_path results/bins-concot
```

# Discutamos

https://docs.google.com/document/d/1iiw-q-90nATg-RNTd9nU8L1XE5xoDRC6j19JC1GiPIk/edit?usp=sharing

### Refinamiento

#### DASTool

Preparing input files.

```{bash, eval=FALSE}
Fasta_to_Scaffolds2Bin.sh -i /home/mirna/05.Concoct/bins-concot -e fa > htn_concot.scaffolds2bin.tsv

Fasta_to_Scaffolds2Bin.sh -i /home/mirna/04.Metabat2 -e fa > htn.scaffolds2bin.tsv
```

```{bash DAS default, eval=FALSE}
PATH=/home/programs:$PATH
/home/programs/DAS_Tool-1.1.2/DAS_Tool -i htn_maxbin.contigs2bin.tsv,htn_metabat.scaffolds2bin.tsv,htn_concoct.scaffolds2bin.tsv -l maxbin,metabat,concoct -c data/htn.fasta -o results/htn_bins --debug -t 4  --search_engine diamond --write_bins 1 
```

### CheckM

Muy bien, primero crea un nuevo directorio y entra al directorio.

```{bash eval=F}
mkdir 06.CheckM
cd 06.CheckM/
```

Ahora activemos el ambiente.

```{bash eval=F}
conda activate checkm_env
```


```{bash, eval=FALSE}
checkm  lineage_wf -t 4 -x fa /home/mirna/05.DAS_tool/results/htn_bins_DASTool_bins DAStools-log_htn  -f CheckM-DAS_Tool_bins.txt
```

Vamos a explorar la salida de checkM

```{r}
library(tidyverse)
# CheckM -------------------------------------------------------------------####
checkm<-read.table(
  "CheckM-DAS_Tool_bins.txt", 
  sep = "", header = F, 
  na.strings ="", stringsAsFactors= F)
# Extracting good quality bins Megahit ------------------------------------####
colnames(checkm)<-c("Bin_Id", "Marker", "lineage", "Number_of_genomes", 
                         "Number_of_markers", "Number_of_marker_sets", 
                         "0", "1", "2", "3", "4", "5", "Completeness", 
                         "Contamination", "Strain_heterogeneity")  

good_bins<-checkm %>%
  select(Bin_Id, Marker, Completeness, Contamination) %>%
  filter(Completeness >= 50.00) %>%
  filter(Contamination <= 10.00) 
```




