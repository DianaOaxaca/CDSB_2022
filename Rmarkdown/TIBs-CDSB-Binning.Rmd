---
title: "TIBs-CDSB-Mapping"
author: "Diana Oaxaca y Mirna Vazquez Rosas Landaa"
date: '2022-07-11'
output: html_document
---

### Mapeo

3. **Profundidad**: La profundidad de cada contig generado se realizó mediante el mapeo de las lecturas al ensamble. Este paso permite evaluar la calidad del ensable y es necesario para hacer la reconstrucción de genomas ya que, como veremos más adelante, es uno de los parámetros que toman en cuenta los bineadores. El mapeo se realizó con la herramienta BBMap del programa **[BBtools](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/)**.

   ```shell
   bash /home/programs/bbmap/bbmap.sh ref=data/htn.fasta in1=data/htn_1.fastq in2=data/htn_2.fastq out=data/htn.sam kfilter=22 subfilter=15 maxindel=80 bamscript=src/sam2bam.sh covstats=data/htn.covstats scafstats=data/htn.scafstats
   bash src//sam2bam.sh
   ```

### Binning

Utilizaremos varios programas para hacer la reconstrucción de los genomas y haremos una comparación de estos.

**NOTA**: Cada programa tiene una ayuda y un manual de usuario, es **importante** revisarlo y conocer cada parámetro que se ejecute. En terminal se puede consultar el manual con el comando `man` y también se puede consultar la ayuda con `-h` o `--help`, por ejemplo `fastqc -h`.

La presente práctica sólo es una representación del flujo de trabajo, sin embargo, no sustituye los manuales de cada programa y el flujo puede variar dependiendo del tipo de datos y pregunta de investigación.

**¡Manos a la obra!**

Conéctate al servidor:

```shell
ssh USER@hpc-matematicas-z.fciencias.unam.mx
```

Crea tu espacio de trabajo y una liga símbólica hacia los datos que se usarán:

# ESTA PARTE ME CAUSA CONFLICTO # La de crear asi los directorios? podríamos quitarla y que hagann su propia estructura de trabajo. Solo en las lineas de comando hay que quitar entonces las rutas relativas para que puedan copiar y pegar la linea sin error


```shell
mkdir -p htn/{data,results,logs}
cd htn
ln -s /home/diana/htn/data/* .  
cd ..
```

#### MaxBin

```{bash run MaxBin2, eval=FALSE}
run_MaxBin.pl -contig data/htn.fasta -out results/maxbin/ -abund data/htn-depth.txt
```
**131 bins, 58.37 min, puede mejorar si se bajan las iteraciones (deafult 50) y se indican los nodos (default 1)**

#### BinSanity

```{bash create normalized coverage file, eval=FALSE}
Binsanity-profile -i data/htn.fasta -s data/ -c results/binsanity/binsanity-profile
```

**esta sección aún no sale**
```{bash run Binsanity-wf, eval=FALSE}
Binsanity-wf -f data/ -l htn.fasta -c results/binsanity/binsanity-profile.cov.x100.lognorm -o results/binsanity/htn-BinsanityWF/
```

#### MetaBat

Para MetaBat lo primero que tenemos que hacer es crear un archivo de profundidad utilizando el script **jgi_summarize_bam_contig_depths**.

Entonces primero activamos el ambinte.

```{bash eval=FALSE}
conda activate metabat
```

Como cualquier otro programa **jgi_summarize_bam_contig_depths** tiene opciones, podemos revisarlas. 

```{bash eval=FALSE}
jgi_summarize_bam_contig_depths  --outputDepth htn-depth.txt htn_sorted.bam
```

Okay... exploremos el archivo con **head**

```{bash eval=FALSE}
head htn-depth.txt
```

Para metabat solo necesitamos dos archivos principales:

- El ensamble
- El archivo de profundidad

El resto de argumentos que vamos a usar se refieren a:

 - minCVSum: Cobertura media  total mínima de un contig (suma de profundidad)
 - saveCls: guardar membresías de clúster como un formato de matriz
 - d: salida corta
 - v: salida detallada
 - minCV: Cobertura media mínima de un contig en cada biblioteca para binning.
 - m: Tamaño mínimo de un contig para binning (debe ser >=1500). Usualmente usamos 2000 pero para el ejercicio usaremos 1500
 
```{bash running Metabat, eval=FALSE}
metabat -i htn.fasta -a /home/mirna/03.Mapeo/htn-depth.txt -o bins -t 1 --minCVSum 0 --saveCls -d -v --minCV 0.1 -m 1500
```

**** TOMO 20 MINUTOS ***
 
#### CONCOCT

Primero activemos el ambiente

```{bash eval=FALSE}
conda activate concoct_env
```

Primero los contigs se tienen que partir en pedazos mas pequeños

```{bash split assembly, eval=FALSE}
cut_up_fasta.py htn.fasta -c 10000 -o 0 --merge_last -b SplitAssembly-htn.bed > htn.fasta-split10K.fa
```

Para creas latabla de cobertura se necesita primero indexar el archivo bam

```{bash index bamfile, eval=FALSE}
samtools index htn_sorted.bam
```

```{bash create coverage table, eval=FALSE}
concoct_coverage_table.py SplitAssembly-htn.bed htn_sorted.bam > concoct_coverage_table_htn.tsv
```

Ahora si!! a correr concoct.

Normalmente correriamos 500 iteraciones, pero esta vez solo haremos una.

```{bash run concot, eval=FALSE}
concoct --coverage_file concoct_coverage_table_htn.tsv --composition_file htn.fasta-split10K.fa --clusters 400 --kmer_length 4 --threads 4 --length_threshold 3000 --basename concot --seed 4 --iterations 1
```

Merge subcontig clustering into original contigs

```{bash merge step, eval=FALSE}
merge_cutup_clustering.py concot_clustering_gt3000.csv > merged-htn-gt3000.csv
```

Extract bins as individual fasta.

```{bash make fastafiles, eval=FALSE}
mkdir bins-concot
extract_fasta_bins.py  htn.fasta merged-htn-gt3000.csv --output_path bins-concot
```

#### Vamb
```{bash activate environment, eval=FALSE}
conda activate /home/mirna/miniconda3/envs/vamb_env
vamb -o _  --outdir results/vamb --fasta data/htn.fasta --jgi htn.depth.txt
```
**el error dice que la columna 5 deben ser varianzas y si lo son, hay un issue que detalla más pasos previos para el archivo depth, no lo entiendo, hay que checarlo.**

### Calidad y limpieza

#### CheckM

#### mmGenome

### Refinamiento

#### DASTool
